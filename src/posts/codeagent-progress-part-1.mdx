---
title: "Building CodeAgent - Concept Check (Part 1): Designing an AI-Powered Quiz Platform"
summary: "A behind-the-scenes look at building CodeAgent â€” an AI-driven quiz generator for computer science students, exploring the project vision, architecture, early challenges, and lessons learned so far."
date: "2025-11-08"
readTime: "7 min read"
slug: "codeagent-progress-part-1"
tags: ["Next.js", "Supabase", "OpenAI", "AI", "Education", "Full-Stack", "Project Journal"]
---

<img src="/posts/codeagent-hero.jpg" alt="CodeAgent prototype mockup" className="my-8 rounded-lg shadow-md" />

Iâ€™ve always been drawn to **education technology** and the idea of helping others learn computer science more effectively.  
From tutoring classmates in college to mentoring hundreds of students as a **CodePath Tech Fellow** and **CodaKid instructor**, Iâ€™ve seen a common challenge:  
many learners want to practice outside of class or structured sessions â€” but they donâ€™t always know *where to start* or *how to test their skills*.

That realization inspired me to start **CodeAgent - Concept Check**, an AI-powered web app designed to generate real-time, personalized computer science quizzes for students.  
It combines everything I love â€” **teaching, building, and experimenting with AI** â€” into one full-stack project.

<hr />

## Project Vision

CodeAgent aims to provide:
- On-demand computer science quizzes generated by AI  
- Topic and difficulty selection (e.g., Data Structures, Web Development, OOP)  
- Detailed explanations for each answer  
- A clean, responsive interface for learning anytime, anywhere  

The long-term goal is to build a **personalized learning companion** that adapts to how each student learns, offering feedback and analytics over time.

{/* ```plaintext
User â†’ selects topic â†’ sends prompt to OpenAI â†’ receives quiz â†’ answers question â†’ sees explanation â†’ stored in Supabase
```` */}

<hr />

## Choosing the Tech Stack

I wanted this project to feel modern, scalable, and cloud-native while still being approachable to other developers or educators who might extend it later.

{/* | Layer          | Tech                  | Why I Chose It                                          |
| -------------- | --------------------- | ------------------------------------------------------- |
| **Frontend**   | Next.js 15            | Clean architecture with App Router and Server Actions   |
| **Backend**    | Supabase              | PostgreSQL + Auth + Edge Functions, great developer DX  |
| **AI**         | OpenAI GPT-4 Turbo    | Generates structured, topic-based questions efficiently |
| **Styling**    | Tailwind CSS + NextUI | Fast to prototype and visually consistent               |
| **Deployment** | Vercel                | Simple deployments and automatic environment variables  | */}

**Frontend:** Next.js 15 â€“ Clean architecture with App Router and Server Actions  
**Backend:** Supabase â€“ PostgreSQL + Auth + Edge Functions  
**AI:** OpenAI GPT-4 Turbo â€“ Structured topic-based question generation  
**Styling:** Tailwind + NextUI â€“ Fast to prototype and visually consistent  
**Deployment:** Vercel â€“ Simple deploys with environment variable support


<hr />

## Designing the Interface

Before writing any code, I created the **layout and flow in Figma**.
The design goal was *simple, modern, and focused*: a centered quiz card, soft color gradients, and a dashboard-style layout for future expansion.

Once I had the outline, I started building the frontend:

* Setting up the Next.js project structure and page routing
* Creating reusable components like `QuizCard`, `TopicSelector`, and `QuestionDisplay`
* Designing a minimal form to select topic and difficulty
* Preparing loading and result states for quiz output

This early focus on structure helps keep the project organized as features grow.

<img src="/posts/codeagent-figma2.png" alt="CodeAgent Figma layout" className="my-8 rounded-lg shadow-md" />

<hr />

## Connecting to OpenAI

The core of CodeAgent is its **AI quiz generation logic**.
I wrote an initial server action to connect to OpenAIâ€™s GPT-4-Turbo model using a dynamic prompt template:

```ts
const prompt = `
Generate one ${difficulty}-level multiple-choice question about ${topic}.
Include 4 options (A-D), specify the correct answer, and provide a short explanation.
Return JSON in this structure:
{
  "question": "...",
  "options": ["A", "B", "C", "D"],
  "answer": "A",
  "explanation": "..."
}
`;
```

Iâ€™m currently in the testing phase â€” refining prompts, validating responses, and rendering them dynamically inside a quiz card component.
The goal is to have the app generate consistent, structured data that can easily be parsed and displayed without manual cleanup.

<hr />

## Architecture Overview

<img src="/posts/codeagent-architecture.png" alt="CodeAgent system architecture diagram" className="my-8 rounded-lg shadow-md w-md mx-auto" />

The architecture focuses on **simplicity and security** for an MVP:

**Frontend**

* Next.js handles all user interactions and form submissions
* Server Actions call OpenAI securely (no client-side API keys)
* Quiz results rendered in real time on a `QuizCard` component

**Backend & Database**

* Supabase stores quiz sessions and topics for tracking
* Edge Functions planned for future quiz analytics
* No authentication for MVP â€” to keep the workflow simple while testing core logic

**AI Layer**

* OpenAI API generates questions, explanations, and correct answers
* Responses validated with Zod (JSON schema) to ensure safe rendering

This modular setup makes it easy to add authentication, history, and adaptive learning in future iterations.

<hr />

## Challenges So Far

### Prompt Engineering

Getting the model to consistently produce *valid JSON* responses was tougher than expected.
Iâ€™ve learned to keep instructions short, enforce schema validation, and test multiple temperature values to balance creativity with accuracy.

### Rendering Dynamic Output

Transforming the OpenAI output into a clean React card UI introduced edge cases â€” like missing options or inconsistent spacing.
Building defensive rendering logic and placeholders improved reliability.

### Balancing Simplicity and Scalability

Itâ€™s tempting to add profiles and analytics early, but I decided to focus on shipping a working MVP first.
This project reminded me that **clarity beats complexity** â€” especially in early development.

<hr />

## Current Stage & Next Steps

Right now, CodeAgent can:

* Select topics and difficulty
* Send prompts to OpenAI and receive structured questions
* Display generated questions and explanations dynamically

Next steps include:

* Adding Supabase storage for quiz history
* Testing prompt refinement for consistent quality
* Introducing basic authentication (Supabase Auth)
* Designing a dashboard to view completed quizzes

<hr />

## Early Takeaways

Even in its early stages, CodeAgent has already taught me:

* How to structure full-stack AI apps with modern frameworks
* Why prompt design is as important as code design
* How to think like both an **engineer** and an **educator**

This project combines everything I care about â€” **learning, technology, and impact**.
I canâ€™t wait to share Part 2 once CodeAgentâ€™s MVP is live with user tracking, progress analytics, and smarter question generation.

> ðŸ’¡ Stay tuned for Part 2 â€” Iâ€™ll be covering Supabase integration, authentication, and insights from real testing sessions.
> ðŸ§  GitHub repo coming soon.


